"""
Ner-based Weighting:

This module implements watermarking and detection functionality for text generated by 
large language models such as LLaMA or GPT-style models. 

Key Features:
- NER-based weighting to adjust contributions of named entities during detection.
- Support for normalization strategies applied to input text.
- Compatibility with Hugging Face Transformers tokenizers and models.

Usage:
Import the classes in the detection script, configure parameters as needed, 
and call the detect() method to analyze text for watermarks.

Author:
Jingmiao Li, adapted and extended from the original lm-watermarking repository (MIT License).
"""

from __future__ import annotations
from math import sqrt
import scipy.stats
import torch
from torch import Tensor
from transformers import LogitsProcessor
from normalizers import normalization_strategy_lookup
import spacy
import torch
from transformers import AutoTokenizer

class WatermarkBase:
    def __init__(
        self,
        vocab: list[int] = None,
        gamma: float = 0.5,
        delta: float = 2.0,
        seeding_scheme: str = "simple_1",  # mostly unused/always default
        hash_key: int = 15485863,  # just a large prime number to create a rng seed with sufficient bit width
        select_green_tokens: bool = True,
    ):

        # watermarking parameters
        self.vocab = vocab
        self.vocab_size = len(vocab)
        self.gamma = gamma
        self.delta = delta
        self.seeding_scheme = seeding_scheme
        self.rng = None
        self.hash_key = hash_key
        self.select_green_tokens = select_green_tokens

    def _seed_rng(self, input_ids: torch.LongTensor, seeding_scheme: str = None) -> None:
        # can optionally override the seeding scheme,
        # but uses the instance attr by default
        if seeding_scheme is None:
            seeding_scheme = self.seeding_scheme

        if seeding_scheme == "simple_1":
            assert input_ids.shape[-1] >= 1, f"seeding_scheme={seeding_scheme} requires at least a 1 token prefix sequence to seed rng"
            prev_token = input_ids[-1].item()
            self.rng.manual_seed(self.hash_key * prev_token)
        else:
            raise NotImplementedError(f"Unexpected seeding_scheme: {seeding_scheme}")
        return

    def _get_greenlist_ids(self, input_ids: torch.LongTensor) -> list[int]:
        # seed the rng using the previous tokens/prefix
        # according to the seeding_scheme
        self._seed_rng(input_ids)

        greenlist_size = int(self.vocab_size * self.gamma)
        vocab_permutation = torch.randperm(self.vocab_size, device=input_ids.device, generator=self.rng)
        if self.select_green_tokens:  # directly
            greenlist_ids = vocab_permutation[:greenlist_size]  # new
        else:  # select green via red
            greenlist_ids = vocab_permutation[(self.vocab_size - greenlist_size) :]  # legacy behavior
        return greenlist_ids


class WatermarkLogitsProcessor(WatermarkBase, LogitsProcessor):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

    def _calc_greenlist_mask(self, scores: torch.FloatTensor, greenlist_token_ids) -> torch.BoolTensor:
        # TODO lets see if we can lose this loop
        green_tokens_mask = torch.zeros_like(scores)
        for b_idx in range(len(greenlist_token_ids)):
            green_tokens_mask[b_idx][greenlist_token_ids[b_idx]] = 1
        final_mask = green_tokens_mask.bool()
        return final_mask

    def _bias_greenlist_logits(self, scores: torch.Tensor, greenlist_mask: torch.Tensor, greenlist_bias: float) -> torch.Tensor:
        scores[greenlist_mask] = scores[greenlist_mask] + greenlist_bias
        return scores

    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:

        # this is lazy to allow us to colocate on the watermarked model's device
        if self.rng is None:
            self.rng = torch.Generator(device=input_ids.device)

        # NOTE, it would be nice to get rid of this batch loop, but currently,
        # the seed and partition operations are not tensor/vectorized, thus
        # each sequence in the batch needs to be treated separately.
        batched_greenlist_ids = [None for _ in range(input_ids.shape[0])]

        for b_idx in range(input_ids.shape[0]):
            greenlist_ids = self._get_greenlist_ids(input_ids[b_idx])
            batched_greenlist_ids[b_idx] = greenlist_ids

        green_tokens_mask = self._calc_greenlist_mask(scores=scores, greenlist_token_ids=batched_greenlist_ids)

        scores = self._bias_greenlist_logits(scores=scores, greenlist_mask=green_tokens_mask, greenlist_bias=self.delta)
        return scores

class WatermarkDetector(WatermarkBase):
    def __init__(
        self,
        *args,
        tokenizer: AutoTokenizer,  
        entity_scale: float = 1.0,
        z_threshold: float = 4.0,
        normalizers: list[str] = ["unicode"],  # or also: ["unicode", "homoglyphs", "truecase"]
        device: torch.device = None,
        use_ner: bool = True,
        **kwargs,
    ):
        super().__init__(*args, **kwargs)
        self.tokenizer = tokenizer
        self.entity_scale = entity_scale
        self.z_threshold = z_threshold
        self.device = device or torch.device("cpu")
        self.use_ner = use_ner
        self.rng = torch.Generator(device=self.device)

        if self.seeding_scheme == "simple_1":
            self.min_prefix_len = 1
        else:
            raise NotImplementedError(f"Unexpected seeding_scheme: {self.seeding_scheme}")

        self.normalizers = []
        for normalization_strategy in normalizers:
            self.normalizers.append(normalization_strategy_lookup(normalization_strategy))

        # 加载 spaCy 模型
        if self.use_ner and not hasattr(self, "ner_model"):
            self.ner_model = spacy.load("en_core_web_md")

    def _align_ner_to_tokens_with_overlap(self, text: str, ner_char_positions: list[tuple[int, int]]) -> list[int]:
        """
        Align NER entities to LLaMA token indices, allowing partial overlap.
        """
        encoding = self.tokenizer(text, return_offsets_mapping=True, add_special_tokens=False)
        token_offsets = encoding["offset_mapping"]
        tokens = self.tokenizer.convert_ids_to_tokens(encoding["input_ids"])

        # Correct token offsets to handle token length adjustments
        corrected_offsets = [
            (start, start + len(token)) if start == end else (start, end)
            for token, (start, end) in zip(tokens, token_offsets)
        ]

        ner_token_indices = []
        for start_char, end_char in ner_char_positions:
            token_indices = [
                idx for idx, (token_start, token_end) in enumerate(corrected_offsets)
                if token_start < end_char and token_end > start_char
            ]
            ner_token_indices.extend(token_indices)
        return list(set(ner_token_indices))  # Remove duplicates


    def _compute_z_score(self, green_token_count: float, num_tokens_scored: float) -> float:
        expected_green_fraction = self.gamma
        expected_green_count = num_tokens_scored * expected_green_fraction
        std_dev = sqrt(num_tokens_scored * expected_green_fraction * (1 - expected_green_fraction))
        z_score = (green_token_count - expected_green_count) / std_dev
        return z_score

    def _compute_p_value(self, z):
        p_value = scipy.stats.norm.sf(z)
        return p_value

    def _score_sequence(
        self,
        input_ids: Tensor,
        ner_token_indices: list[int] = None,
    ) -> dict:
        """
        Score tokens, adjusting weights for NER tokens.
        """
        num_tokens_scored = 0
        green_token_count = 0
        green_token_mask = []

        for idx in range(self.min_prefix_len, len(input_ids)):
            curr_token = input_ids[idx]
            greenlist_ids = self._get_greenlist_ids(input_ids[:idx])
            is_green = curr_token in greenlist_ids

            if curr_token in self.tokenizer.all_special_ids:
                continue

            # Adjusting NER weights according to index
            weight = self.entity_scale if ner_token_indices and idx in ner_token_indices else 1.0
            num_tokens_scored += weight
            if is_green:
                green_token_count += weight
                green_token_mask.append(True)
            else:
                green_token_mask.append(False)

        green_fraction = green_token_count / num_tokens_scored if num_tokens_scored > 0 else 0
        z_score = self._compute_z_score(green_token_count, num_tokens_scored)
        p_value = self._compute_p_value(z_score)

        return {
            "num_tokens_scored": num_tokens_scored,
            "num_green_tokens": green_token_count,
            "green_fraction": green_fraction,
            "z_score": z_score,
            "p_value": p_value,
            "prediction": z_score > self.z_threshold,
            "green_token_mask": green_token_mask,
        }

    def detect(self, text: str = None, **kwargs) -> dict:
        """
        Detect watermark with NER and token weighting.
        """
        if not text:
            raise ValueError("Text input is required for detection.")
        
        # run optional normalizers on text
        for normalizer in self.normalizers:
            text = normalizer(text)
        if len(self.normalizers) > 0:
            print(f"Text after normalization:\n\n{text}\n")

        # Tokenize text
        encoding = self.tokenizer(text, return_tensors="pt", add_special_tokens=False)
        tokenized_text = encoding["input_ids"][0].to(self.device)

        # Obtain character idx of NER 
        ner_token_indices, ner_entities = [], []
        if self.use_ner:
            doc = self.ner_model(text)
            ner_char_positions = [(ent.start_char, ent.end_char) for ent in doc.ents]
            ner_token_indices = self._align_ner_to_tokens_with_overlap(text, ner_char_positions)
            ner_entities = [(ent.text, ent.start_char, ent.end_char) for ent in doc.ents]

        # score 
        results = self._score_sequence(tokenized_text, ner_token_indices)
        results["ner_entities"] = ner_entities 
        return results
